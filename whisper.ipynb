{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ad34fe1",
   "metadata": {},
   "source": [
    "# Real-Time Speech-to-Text & Text-to-Speech\n",
    "This notebook provides production-ready, real-time functions for Text-to-Speech (streaming directly to speakers) and Speech-to-Text (recording from microphone and transcribing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2569d874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import time\n",
    "import pyaudio\n",
    "import speech_recognition as sr\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables (e.g., OPENAI_API_KEY from .env)\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI Client\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b191858b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_speech_stream(text: str) -> None:\n",
    "    \"\"\"\n",
    "    Converts text to speech using OpenAI's TTS API and streams it directly to the speakers.\n",
    "    Using the 'pcm' response format allows us to stream audio real-time without saving to a file.\n",
    "    \"\"\"\n",
    "    print(\"\\n[TTS] Generating and streaming audio...\")\n",
    "    \n",
    "    # Request a streaming response\n",
    "    with client.audio.speech.with_streaming_response.create(\n",
    "        model=\"tts-1\",\n",
    "        voice=\"alloy\",\n",
    "        input=text,\n",
    "        response_format=\"pcm\"\n",
    "    ) as response:\n",
    "        \n",
    "        # Initialize PyAudio\n",
    "        p = pyaudio.PyAudio()\n",
    "        \n",
    "        # OpenAI TTS PCM format is 24kHz, 1 channel, 16-bit\n",
    "        stream = p.open(format=pyaudio.paInt16,\n",
    "                        channels=1,\n",
    "                        rate=24000,\n",
    "                        output=True)\n",
    "        \n",
    "        # Stream bytes directly to the audio output device\n",
    "        for chunk in response.iter_bytes(chunk_size=1024):\n",
    "            if chunk:\n",
    "                stream.write(chunk)\n",
    "                \n",
    "        # Clean up\n",
    "        time.sleep(0.1) # Small delay to ensure last chunk plays fully\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        p.terminate()\n",
    "        \n",
    "    print(\"[TTS] Finished speaking.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e58a9e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_and_transcribe() -> str:\n",
    "    \"\"\"\n",
    "    Records audio from the microphone until silence is detected, \n",
    "    then transcribes it using OpenAI's Whisper model.\n",
    "    \"\"\"\n",
    "    r = sr.Recognizer()\n",
    "    r.energy_threshold = 300\n",
    "    r.dynamic_energy_threshold = True\n",
    "    \n",
    "    with sr.Microphone() as source:\n",
    "        print(\"\\n[STT] Adjusting for ambient noise... Please wait.\")\n",
    "        r.adjust_for_ambient_noise(source, duration=0.8)\n",
    "        print(\"[STT] \\033[92mListening... (Speak now. It will stop automatically when you stop speaking.)\\033[0m\")\n",
    "        \n",
    "        try:\n",
    "            # Listen for up to 15 seconds of audio\n",
    "            audio = r.listen(source, timeout=5, phrase_time_limit=15)\n",
    "        except sr.WaitTimeoutError:\n",
    "            print(\"[STT] Listening timed out while waiting for phrase to start\")\n",
    "            return \"\"\n",
    "            \n",
    "    print(\"[STT] \\033[93mRecording complete. Transcribing...\\033[0m\")\n",
    "    \n",
    "    # Get in-memory WAV data\n",
    "    wav_data = audio.get_wav_data()\n",
    "    \n",
    "    # Create an in-memory file-like object\n",
    "    audio_file = io.BytesIO(wav_data)\n",
    "    audio_file.name = \"audio.wav\"\n",
    "    \n",
    "    # Send to OpenAI Whisper API\n",
    "    transcription = client.audio.transcriptions.create(\n",
    "        model=\"whisper-1\", \n",
    "        file=audio_file, \n",
    "        response_format=\"text\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\n[STT] \\033[96mTranscription:\\n\\033[0m\", transcription)\n",
    "    return transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fe4c15",
   "metadata": {},
   "source": [
    "### Test the Functions\n",
    "Run the cells below to execute the real-time Text-to-Speech and Speech-to-Text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dd9ce1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[TTS] Generating and streaming audio...\n",
      "[TTS] Finished speaking.\n"
     ]
    }
   ],
   "source": [
    "# 1. Test Text-To-Speech Stream\n",
    "text_to_speech_stream(\"Hello, I am ready to convert your speech to text and talk back in real time.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b849857f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[STT] Adjusting for ambient noise... Please wait.\n",
      "[STT] \u001b[92mListening... (Speak now. It will stop automatically when you stop speaking.)\u001b[0m\n",
      "[STT] \u001b[93mRecording complete. Transcribing...\u001b[0m\n",
      "\n",
      "[STT] \u001b[96mTranscription:\n",
      "\u001b[0m Â¡Muy amiguitos mios!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Test Speech-To-Text\n",
    "user_input = record_and_transcribe()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
